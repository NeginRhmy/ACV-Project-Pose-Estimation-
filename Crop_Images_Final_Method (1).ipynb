{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTP4-WUczcfS"
      },
      "source": [
        "# Patching with Writes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fB1_gLA7xbrj"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "import json\n",
        "import os\n",
        "\n",
        "\n",
        "def detect_corners_harris(gray, block_size=2, ksize=3, k=0.04, threshold_ratio=0.01):\n",
        "    gray_float = np.float32(gray)\n",
        "    dst = cv2.cornerHarris(gray_float, blockSize=block_size, ksize=ksize, k=k)\n",
        "    corner_mask = dst > threshold_ratio * dst.max()\n",
        "    return corner_mask, dst\n",
        "\n",
        "\n",
        "def count_corners_in_region(corner_mask, x, y, w, h):\n",
        "    region = corner_mask[y:y+h, x:x+w]\n",
        "    return np.sum(region)\n",
        "\n",
        "\n",
        "def compute_corner_density(corner_mask, x, y, w, h):\n",
        "    area = max(w * h, 1)\n",
        "    count = count_corners_in_region(corner_mask, x, y, w, h)\n",
        "    return count / area\n",
        "\n",
        "\n",
        "def adaptive_subdivide(corner_mask, x, y, w, h,\n",
        "                       density_threshold=0.005,\n",
        "                       min_patch_width=32,\n",
        "                       max_patch_width=256,\n",
        "                       aspect_ratio=2.0):\n",
        "    min_patch_height = int(min_patch_width * aspect_ratio)\n",
        "\n",
        "    if w <= min_patch_width or h <= min_patch_height:\n",
        "        return [(x, y, w, h)]\n",
        "\n",
        "    density = compute_corner_density(corner_mask, x, y, w, h)\n",
        "\n",
        "    min_w_check = max_patch_width // 2\n",
        "    min_h_check = int(min_w_check * aspect_ratio)\n",
        "\n",
        "    if density <= density_threshold or \\\n",
        "       (w <= min_w_check and h <= min_h_check and density <= density_threshold * 2):\n",
        "        return [(x, y, w, h)]\n",
        "\n",
        "    half_w = w // 2\n",
        "    half_h = h // 2\n",
        "    right_w = w - half_w\n",
        "    bottom_h = h - half_h\n",
        "\n",
        "    patches = []\n",
        "    patches += adaptive_subdivide(corner_mask, x, y,\n",
        "                                   half_w, half_h,\n",
        "                                   density_threshold, min_patch_width,\n",
        "                                   max_patch_width, aspect_ratio)\n",
        "    patches += adaptive_subdivide(corner_mask, x + half_w, y,\n",
        "                                   right_w, half_h,\n",
        "                                   density_threshold, min_patch_width,\n",
        "                                   max_patch_width, aspect_ratio)\n",
        "    patches += adaptive_subdivide(corner_mask, x, y + half_h,\n",
        "                                   half_w, bottom_h,\n",
        "                                   density_threshold, min_patch_width,\n",
        "                                   max_patch_width, aspect_ratio)\n",
        "    patches += adaptive_subdivide(corner_mask, x + half_w, y + half_h,\n",
        "                                   right_w, bottom_h,\n",
        "                                   density_threshold, min_patch_width,\n",
        "                                   max_patch_width, aspect_ratio)\n",
        "    return patches\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "#              EDGE-AWARE MERGING\n",
        "# ============================================================\n",
        "\n",
        "def compute_edge_map(gray):\n",
        "    blurred = cv2.GaussianBlur(gray, (3, 3), 0)\n",
        "    edges = cv2.Canny(blurred, 50, 150)\n",
        "    return edges\n",
        "\n",
        "\n",
        "def get_shared_boundary(patch_a, patch_b):\n",
        "    ax, ay, aw, ah = patch_a\n",
        "    bx, by, bw, bh = patch_b\n",
        "    tolerance = 2\n",
        "\n",
        "    if abs((ay + ah) - by) <= tolerance:\n",
        "        ol = max(ax, bx)\n",
        "        or_ = min(ax + aw, bx + bw)\n",
        "        if or_ - ol > tolerance:\n",
        "            return ('horizontal', ol, or_, ay + ah)\n",
        "\n",
        "    if abs((by + bh) - ay) <= tolerance:\n",
        "        ol = max(ax, bx)\n",
        "        or_ = min(ax + aw, bx + bw)\n",
        "        if or_ - ol > tolerance:\n",
        "            return ('horizontal', ol, or_, by + bh)\n",
        "\n",
        "    if abs((ax + aw) - bx) <= tolerance:\n",
        "        ot = max(ay, by)\n",
        "        ob = min(ay + ah, by + bh)\n",
        "        if ob - ot > tolerance:\n",
        "            return ('vertical', ot, ob, ax + aw)\n",
        "\n",
        "    if abs((bx + bw) - ax) <= tolerance:\n",
        "        ot = max(ay, by)\n",
        "        ob = min(ay + ah, by + bh)\n",
        "        if ob - ot > tolerance:\n",
        "            return ('vertical', ot, ob, bx + bw)\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def compute_boundary_edge_density(edges, boundary_info, band_width=5):\n",
        "    orientation, start, end, line_pos = boundary_info\n",
        "    h, w = edges.shape\n",
        "\n",
        "    if orientation == 'horizontal':\n",
        "        rs = max(0, line_pos - band_width)\n",
        "        re = min(h, line_pos + band_width)\n",
        "        band = edges[rs:re, start:end]\n",
        "    else:\n",
        "        cs = max(0, line_pos - band_width)\n",
        "        ce = min(w, line_pos + band_width)\n",
        "        band = edges[start:end, cs:ce]\n",
        "\n",
        "    if band.size == 0:\n",
        "        return 0.0\n",
        "    return np.sum(band > 0) / band.size\n",
        "\n",
        "\n",
        "def merged_patch_bbox(patch_a, patch_b):\n",
        "    ax, ay, aw, ah = patch_a\n",
        "    bx, by, bw, bh = patch_b\n",
        "    new_x = min(ax, bx)\n",
        "    new_y = min(ay, by)\n",
        "    new_x2 = max(ax + aw, bx + bw)\n",
        "    new_y2 = max(ay + ah, by + bh)\n",
        "    return (new_x, new_y, new_x2 - new_x, new_y2 - new_y)\n",
        "\n",
        "\n",
        "def should_merge(patch_a, patch_b, edges, max_patch_width, max_patch_height,\n",
        "                 edge_density_threshold=0.15, band_width=5):\n",
        "    boundary = get_shared_boundary(patch_a, patch_b)\n",
        "    if boundary is None:\n",
        "        return False, None, 0.0\n",
        "\n",
        "    merged = merged_patch_bbox(patch_a, patch_b)\n",
        "    _, _, mw, mh = merged\n",
        "\n",
        "    if mw > max_patch_width or mh > max_patch_height:\n",
        "        return False, boundary, 0.0\n",
        "\n",
        "    edge_density = compute_boundary_edge_density(edges, boundary, band_width)\n",
        "\n",
        "    if edge_density > edge_density_threshold:\n",
        "        return True, boundary, edge_density\n",
        "\n",
        "    return False, boundary, edge_density\n",
        "\n",
        "\n",
        "def edge_aware_merge(patches, edges, max_patch_width, max_patch_height,\n",
        "                     edge_density_threshold=0.15,\n",
        "                     band_width=5,\n",
        "                     max_iterations=50):\n",
        "    merged_patches = list(patches)\n",
        "    total_merges = 0\n",
        "\n",
        "    for iteration in range(max_iterations):\n",
        "        best_merge = None\n",
        "        best_density = 0.0\n",
        "\n",
        "        for i in range(len(merged_patches)):\n",
        "            for j in range(i + 1, len(merged_patches)):\n",
        "                do_merge, boundary, density = should_merge(\n",
        "                    merged_patches[i], merged_patches[j],\n",
        "                    edges, max_patch_width, max_patch_height,\n",
        "                    edge_density_threshold, band_width\n",
        "                )\n",
        "                if do_merge and density > best_density:\n",
        "                    best_merge = (i, j)\n",
        "                    best_density = density\n",
        "\n",
        "        if best_merge is None:\n",
        "            break\n",
        "\n",
        "        i, j = best_merge\n",
        "        new_patch = merged_patch_bbox(merged_patches[i], merged_patches[j])\n",
        "        merged_patches.pop(j)\n",
        "        merged_patches.pop(i)\n",
        "        merged_patches.append(new_patch)\n",
        "        total_merges += 1\n",
        "\n",
        "    print(f\"  Edge-aware merging: {total_merges} merges in {iteration+1} iterations\")\n",
        "    print(f\"  Patches: {len(patches)} → {len(merged_patches)}\")\n",
        "    return merged_patches\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "#         PATCH EXPORT & RECONSTRUCTION (NEW)\n",
        "# ============================================================\n",
        "\n",
        "def export_patches(image, patches, patch_info, output_dir='patches',\n",
        "                   image_path=None):\n",
        "    \"\"\"\n",
        "    Export each patch as an image file + metadata JSON for reconstruction.\n",
        "\n",
        "    Directory structure:\n",
        "      output_dir/\n",
        "        metadata.json        ← coordinates & original image info\n",
        "        patch_000.png        ← cropped patch image\n",
        "        patch_001.png\n",
        "        ...\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    height, width = image.shape[:2]\n",
        "    channels = image.shape[2] if len(image.shape) == 3 else 1\n",
        "\n",
        "    metadata = {\n",
        "        'original_image': image_path if image_path else 'unknown',\n",
        "        'original_width': width,\n",
        "        'original_height': height,\n",
        "        'original_channels': channels,\n",
        "        'num_patches': len(patches),\n",
        "        'patches': []\n",
        "    }\n",
        "\n",
        "    for i, (x, y, w, h) in enumerate(patches):\n",
        "        # Crop the patch from the original image\n",
        "        patch_img = image[y:y+h, x:x+w].copy()\n",
        "\n",
        "        # Save patch image\n",
        "        filename = f\"patch_{i:03d}.png\"\n",
        "        filepath = os.path.join(output_dir, filename)\n",
        "        cv2.imwrite(filepath, patch_img)\n",
        "\n",
        "        # Find corresponding patch_info\n",
        "        info = None\n",
        "        for p in patch_info:\n",
        "            if p['x'] == x and p['y'] == y and p['w'] == w and p['h'] == h:\n",
        "                info = p\n",
        "                break\n",
        "\n",
        "        # Store metadata\n",
        "        patch_meta = {\n",
        "            'index': i,\n",
        "            'filename': filename,\n",
        "            'x': int(x),\n",
        "            'y': int(y),\n",
        "            'w': int(w),\n",
        "            'h': int(h),\n",
        "            'density': float(info['density']) if info else 0.0,\n",
        "            'corner_count': int(info['corner_count']) if info else 0\n",
        "        }\n",
        "        metadata['patches'].append(patch_meta)\n",
        "\n",
        "    # Save metadata\n",
        "    meta_path = os.path.join(output_dir, 'metadata.json')\n",
        "    with open(meta_path, 'w') as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "\n",
        "    print(f\"\\n  Exported {len(patches)} patches to '{output_dir}/'\")\n",
        "    print(f\"  Metadata saved to '{meta_path}'\")\n",
        "    print(f\"  Total files: {len(patches)} images + 1 metadata JSON\")\n",
        "\n",
        "    # Print file sizes\n",
        "    total_size = 0\n",
        "    for i in range(len(patches)):\n",
        "        fpath = os.path.join(output_dir, f\"patch_{i:03d}.png\")\n",
        "        fsize = os.path.getsize(fpath)\n",
        "        total_size += fsize\n",
        "\n",
        "    meta_size = os.path.getsize(meta_path)\n",
        "    total_size += meta_size\n",
        "    print(f\"  Total size: {total_size / 1024:.1f} KB \"\n",
        "          f\"(patches: {(total_size - meta_size) / 1024:.1f} KB, \"\n",
        "          f\"metadata: {meta_size / 1024:.1f} KB)\")\n",
        "\n",
        "    return metadata\n",
        "\n",
        "\n",
        "def reconstruct_from_patches(patch_dir='patches', show_result=True):\n",
        "    \"\"\"\n",
        "    Reconstruct the original image from exported patches + metadata.\n",
        "\n",
        "    Reads metadata.json to know:\n",
        "      - Original image dimensions\n",
        "      - Exact (x, y, w, h) for each patch\n",
        "\n",
        "    Then pastes each patch image at its exact location.\n",
        "    \"\"\"\n",
        "    # Load metadata\n",
        "    meta_path = os.path.join(patch_dir, 'metadata.json')\n",
        "    if not os.path.exists(meta_path):\n",
        "        raise FileNotFoundError(f\"Metadata not found at {meta_path}\")\n",
        "\n",
        "    with open(meta_path, 'r') as f:\n",
        "        metadata = json.load(f)\n",
        "\n",
        "    orig_w = metadata['original_width']\n",
        "    orig_h = metadata['original_height']\n",
        "    orig_c = metadata['original_channels']\n",
        "    num_patches = metadata['num_patches']\n",
        "\n",
        "    print(f\"Reconstructing image: {orig_w} x {orig_h} x {orig_c}\")\n",
        "    print(f\"Using {num_patches} patches from '{patch_dir}/'\")\n",
        "\n",
        "    # Create empty canvas\n",
        "    if orig_c == 1:\n",
        "        canvas = np.zeros((orig_h, orig_w), dtype=np.uint8)\n",
        "    else:\n",
        "        canvas = np.zeros((orig_h, orig_w, orig_c), dtype=np.uint8)\n",
        "\n",
        "    # Optional: track coverage to verify no gaps/overlaps\n",
        "    coverage = np.zeros((orig_h, orig_w), dtype=np.int32)\n",
        "\n",
        "    # Paste each patch\n",
        "    for patch_meta in metadata['patches']:\n",
        "        filename = patch_meta['filename']\n",
        "        x = patch_meta['x']\n",
        "        y = patch_meta['y']\n",
        "        w = patch_meta['w']\n",
        "        h = patch_meta['h']\n",
        "\n",
        "        # Load patch image\n",
        "        patch_path = os.path.join(patch_dir, filename)\n",
        "        if not os.path.exists(patch_path):\n",
        "            print(f\"  WARNING: Missing patch file {patch_path}\")\n",
        "            continue\n",
        "\n",
        "        patch_img = cv2.imread(patch_path, cv2.IMREAD_UNCHANGED)\n",
        "\n",
        "        if patch_img is None:\n",
        "            print(f\"  WARNING: Could not read {patch_path}\")\n",
        "            continue\n",
        "\n",
        "        # Verify dimensions match metadata\n",
        "        if orig_c == 1 and len(patch_img.shape) == 3:\n",
        "            patch_img = cv2.cvtColor(patch_img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        patch_h, patch_w = patch_img.shape[:2]\n",
        "        if patch_w != w or patch_h != h:\n",
        "            print(f\"  WARNING: Patch {filename} size mismatch: \"\n",
        "                  f\"expected {w}x{h}, got {patch_w}x{patch_h}\")\n",
        "            # Crop/pad to expected size just in case\n",
        "            patch_img = patch_img[:h, :w]\n",
        "\n",
        "        # Paste onto canvas at exact coordinates\n",
        "        canvas[y:y+h, x:x+w] = patch_img\n",
        "        coverage[y:y+h, x:x+w] += 1\n",
        "\n",
        "    # Check coverage\n",
        "    uncovered = np.sum(coverage == 0)\n",
        "    overlapped = np.sum(coverage > 1)\n",
        "    total_pixels = orig_w * orig_h\n",
        "\n",
        "    print(f\"\\n  Coverage analysis:\")\n",
        "    print(f\"    Covered exactly once: {np.sum(coverage == 1)} / {total_pixels} \"\n",
        "          f\"({100 * np.sum(coverage == 1) / total_pixels:.2f}%)\")\n",
        "    if uncovered > 0:\n",
        "        print(f\"    Uncovered pixels: {uncovered} \"\n",
        "              f\"({100 * uncovered / total_pixels:.2f}%)\")\n",
        "    if overlapped > 0:\n",
        "        print(f\"    Overlapping pixels: {overlapped} \"\n",
        "              f\"({100 * overlapped / total_pixels:.2f}%)\")\n",
        "\n",
        "    if show_result:\n",
        "        print(\"\\n--- Reconstructed Image ---\")\n",
        "        cv2_imshow(canvas)\n",
        "\n",
        "    return canvas, coverage, metadata\n",
        "\n",
        "\n",
        "def verify_reconstruction(original_image_path, patch_dir='patches'):\n",
        "    \"\"\"\n",
        "    Verify that reconstruction is pixel-perfect by comparing\n",
        "    with the original image.\n",
        "    \"\"\"\n",
        "    original = cv2.imread(original_image_path)\n",
        "    if original is None:\n",
        "        raise ValueError(f\"Cannot load original: {original_image_path}\")\n",
        "\n",
        "    reconstructed, coverage, metadata = reconstruct_from_patches(\n",
        "        patch_dir, show_result=False\n",
        "    )\n",
        "\n",
        "    # Pixel-by-pixel comparison\n",
        "    if original.shape != reconstructed.shape:\n",
        "        print(f\"\\n  SHAPE MISMATCH: original {original.shape} vs \"\n",
        "              f\"reconstructed {reconstructed.shape}\")\n",
        "        return False\n",
        "\n",
        "    diff = cv2.absdiff(original, reconstructed)\n",
        "    max_diff = np.max(diff)\n",
        "    mean_diff = np.mean(diff)\n",
        "    num_different = np.sum(diff > 0)\n",
        "    total_values = diff.size\n",
        "\n",
        "    print(f\"\\n  Verification:\")\n",
        "    print(f\"    Max pixel difference: {max_diff}\")\n",
        "    print(f\"    Mean pixel difference: {mean_diff:.6f}\")\n",
        "    print(f\"    Different values: {num_different} / {total_values}\")\n",
        "\n",
        "    if max_diff == 0:\n",
        "        print(f\"    ✅ PERFECT reconstruction! Zero difference.\")\n",
        "        is_perfect = True\n",
        "    else:\n",
        "        print(f\"    ⚠️  Some differences detected (likely PNG compression)\")\n",
        "        is_perfect = False\n",
        "\n",
        "    # Show comparison\n",
        "    print(\"\\n--- Original vs Reconstructed ---\")\n",
        "\n",
        "    # Side by side (resize if too large)\n",
        "    h, w = original.shape[:2]\n",
        "    scale = min(1.0, 800 / w)\n",
        "    if scale < 1.0:\n",
        "        orig_small = cv2.resize(original, None, fx=scale, fy=scale)\n",
        "        recon_small = cv2.resize(reconstructed, None, fx=scale, fy=scale)\n",
        "    else:\n",
        "        orig_small = original\n",
        "        recon_small = reconstructed\n",
        "\n",
        "    comparison = np.hstack([orig_small, recon_small])\n",
        "    cv2.putText(comparison, \"Original\", (10, 30),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "    cv2.putText(comparison, \"Reconstructed\", (orig_small.shape[1] + 10, 30),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "    cv2_imshow(comparison)\n",
        "\n",
        "    # Show difference map (amplified)\n",
        "    if max_diff > 0:\n",
        "        diff_amplified = (diff * (255.0 / max(max_diff, 1))).astype(np.uint8)\n",
        "        print(\"\\n--- Difference Map (amplified) ---\")\n",
        "        cv2_imshow(diff_amplified)\n",
        "\n",
        "    return is_perfect\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "#                    MAIN PIPELINE\n",
        "# ============================================================\n",
        "\n",
        "def generate_adaptive_patches(image_path,\n",
        "                               density_threshold=0.005,\n",
        "                               min_patch_width=32,\n",
        "                               max_patch_width=256,\n",
        "                               aspect_ratio=2.0,\n",
        "                               harris_threshold_ratio=0.01,\n",
        "                               edge_merge_threshold=0.15,\n",
        "                               edge_band_width=5,\n",
        "                               output_dir='patches'):\n",
        "    \"\"\"\n",
        "    Full pipeline: detect → subdivide → merge → export → verify.\n",
        "    \"\"\"\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        raise ValueError(f\"Image not found at {image_path}\")\n",
        "\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    height, width = gray.shape\n",
        "\n",
        "    max_patch_height = int(max_patch_width * aspect_ratio)\n",
        "\n",
        "    print(f\"Image size: {width} x {height}\")\n",
        "    print(f\"Patch size range: {min_patch_width}x{int(min_patch_width * aspect_ratio)} \"\n",
        "          f\"to {max_patch_width}x{max_patch_height}\")\n",
        "\n",
        "    # ── Step 1: Harris Corners ──────────────────────────────\n",
        "    print(\"\\n[Step 1] Detecting Harris corners...\")\n",
        "    corner_mask, dst = detect_corners_harris(gray, threshold_ratio=harris_threshold_ratio)\n",
        "    print(f\"  Total corners detected: {np.sum(corner_mask)}\")\n",
        "\n",
        "    # ── Step 2: Adaptive Subdivision ────────────────────────\n",
        "    print(\"\\n[Step 2] Adaptive subdivision (vertical rectangles)...\")\n",
        "    patches = adaptive_subdivide(\n",
        "        corner_mask,\n",
        "        x=0, y=0, w=width, h=height,\n",
        "        density_threshold=density_threshold,\n",
        "        min_patch_width=min_patch_width,\n",
        "        max_patch_width=max_patch_width,\n",
        "        aspect_ratio=aspect_ratio\n",
        "    )\n",
        "    print(f\"  Patches after subdivision: {len(patches)}\")\n",
        "\n",
        "    # ── Step 3: Edge-Aware Merging ──────────────────────────\n",
        "    print(\"\\n[Step 3] Edge-aware merging...\")\n",
        "    edges = compute_edge_map(gray)\n",
        "    patches_before = len(patches)\n",
        "\n",
        "    patches = edge_aware_merge(\n",
        "        patches, edges,\n",
        "        max_patch_width=max_patch_width,\n",
        "        max_patch_height=max_patch_height,\n",
        "        edge_density_threshold=edge_merge_threshold,\n",
        "        band_width=edge_band_width\n",
        "    )\n",
        "\n",
        "    # ── Step 4: Build patch info ────────────────────────────\n",
        "    patch_info = []\n",
        "    for (x, y, w, h) in patches:\n",
        "        density = compute_corner_density(corner_mask, x, y, w, h)\n",
        "        count = count_corners_in_region(corner_mask, x, y, w, h)\n",
        "        patch_info.append({\n",
        "            'x': x, 'y': y, 'w': w, 'h': h,\n",
        "            'density': density,\n",
        "            'corner_count': count,\n",
        "            'area': w * h,\n",
        "            'aspect': h / max(w, 1)\n",
        "        })\n",
        "    patch_info.sort(key=lambda p: p['density'], reverse=True)\n",
        "\n",
        "    # ── Step 5: Visualization ───────────────────────────────\n",
        "    visualize_results(image, corner_mask, edges, patches, patch_info, patches_before)\n",
        "\n",
        "    # ── Step 6: Export Patches ──────────────────────────────\n",
        "    print(\"\\n[Step 6] Exporting patches...\")\n",
        "    metadata = export_patches(image, patches, patch_info,\n",
        "                              output_dir=output_dir,\n",
        "                              image_path=image_path)\n",
        "\n",
        "    # ── Step 7: Verify Reconstruction ───────────────────────\n",
        "    print(\"\\n[Step 7] Verifying reconstruction...\")\n",
        "    is_perfect = verify_reconstruction(image_path, patch_dir=output_dir)\n",
        "\n",
        "    return patches, patch_info, corner_mask, metadata\n",
        "\n",
        "\n",
        "def visualize_results(image, corner_mask, edges, patches, patch_info, patches_before):\n",
        "\n",
        "    densities = [p['density'] for p in patch_info]\n",
        "    max_density = max(densities) if max(densities) > 0 else 1\n",
        "\n",
        "    def density_color(ratio):\n",
        "        if ratio < 0.5:\n",
        "            r, g, b = int(255 * ratio * 2), 255, 0\n",
        "        else:\n",
        "            r, g, b = 255, int(255 * (1 - (ratio - 0.5) * 2)), 0\n",
        "        return (b, g, r)\n",
        "\n",
        "    # Harris Corners\n",
        "    corners_vis = image.copy()\n",
        "    dst_dilated = cv2.dilate(corner_mask.astype(np.uint8) * 255, None)\n",
        "    corners_vis[dst_dilated > 0] = [0, 0, 255]\n",
        "    print(\"\\n--- Harris Corners ---\")\n",
        "    cv2_imshow(corners_vis)\n",
        "\n",
        "    # Patches (density colored)\n",
        "    patches_vis = image.copy()\n",
        "    for p in patch_info:\n",
        "        x, y, w, h = p['x'], p['y'], p['w'], p['h']\n",
        "        ratio = min(p['density'] / max_density, 1.0)\n",
        "        cv2.rectangle(patches_vis, (x, y), (x+w, y+h), density_color(ratio), 2)\n",
        "    print(f\"\\n--- Patches ({patches_before}→{len(patches)}) ---\")\n",
        "    cv2_imshow(patches_vis)\n",
        "\n",
        "    # Heatmap\n",
        "    alpha_layer = image.copy()\n",
        "    for p in patch_info:\n",
        "        x, y, w, h = p['x'], p['y'], p['w'], p['h']\n",
        "        ratio = min(p['density'] / max_density, 1.0)\n",
        "        cv2.rectangle(alpha_layer, (x, y), (x+w, y+h), density_color(ratio), -1)\n",
        "    result = cv2.addWeighted(image, 0.6, alpha_layer, 0.4, 0)\n",
        "    for p in patch_info:\n",
        "        cv2.rectangle(result, (p['x'], p['y']),\n",
        "                      (p['x']+p['w'], p['y']+p['h']), (255, 255, 255), 1)\n",
        "    print(\"\\n--- Heatmap ---\")\n",
        "    cv2_imshow(result)\n",
        "\n",
        "    # Sizes\n",
        "    size_vis = image.copy()\n",
        "    areas = [p['area'] for p in patch_info]\n",
        "    min_a, max_a = min(areas), max(areas)\n",
        "    ar = max(max_a - min_a, 1)\n",
        "    for p in patch_info:\n",
        "        x, y, w, h = p['x'], p['y'], p['w'], p['h']\n",
        "        ratio = (p['area'] - min_a) / ar\n",
        "        cv2.rectangle(size_vis, (x, y), (x+w, y+h),\n",
        "                      (int(255*ratio), 50, int(255*(1-ratio))), 2)\n",
        "        if w > 40 and h > 30:\n",
        "            cv2.putText(size_vis, f\"{w}x{h}\", (x+3, y+15),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1)\n",
        "    print(\"\\n--- Sizes ---\")\n",
        "    cv2_imshow(size_vis)\n",
        "\n",
        "    print(f\"\\n--- Stats ---\")\n",
        "    print(f\"Patches: {len(patches)}, Merged: {patches_before - len(patches)}\")\n",
        "    print(f\"Non-square: {sum(1 for p in patch_info if p['w'] != p['h'])}\")\n",
        "    print(f\"Avg aspect: {np.mean([p['aspect'] for p in patch_info]):.2f}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3YN80fQtFnqi",
        "outputId": "6f8e20f0-151d-41b4-988e-e245dd526979"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "#                       RUN IT\n",
        "# ============================================================\n",
        "\n",
        "# === STEP 1: Generate, export, and verify ===\n",
        "patches, patch_info, corner_mask, metadata = generate_adaptive_patches(\n",
        "    image_path='/content/img0.png',\n",
        "    density_threshold=0.03,\n",
        "    min_patch_width=32,\n",
        "    max_patch_width=256,\n",
        "    aspect_ratio=2.0,\n",
        "    harris_threshold_ratio=0.01,\n",
        "    edge_merge_threshold=0.15,\n",
        "    edge_band_width=5,\n",
        "    output_dir='/content/patches'\n",
        ")\n",
        "\n",
        "# # === STEP 2: Later, reconstruct from patches directory ===\n",
        "# print(\"\\n\" + \"=\"*60)\n",
        "# print(\"RECONSTRUCTION FROM FILES\")\n",
        "# print(\"=\"*60)\n",
        "# reconstructed, coverage, meta = reconstruct_from_patches(\n",
        "#     patch_dir='patches',\n",
        "#     show_result=True\n",
        "# )\n",
        "\n",
        "# # Save reconstructed image\n",
        "# cv2.imwrite('reconstructed.png', reconstructed)\n",
        "# print(\"Saved reconstructed.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-BRpm7CQfKl"
      },
      "source": [
        "# Setup Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWnDwUm4qDH5",
        "outputId": "b68a0722-f93b-4522-e646-2ac60cbe9513"
      },
      "outputs": [],
      "source": [
        "# --- System deps ---\n",
        "!apt-get update\n",
        "!apt-get install -y swig\n",
        "\n",
        "# --- Python deps ---\n",
        "!pip install gdown\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQpfBKz0qGGf",
        "outputId": "f37482e8-2005-486d-ca86-d7c234744bbf"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/tensorboy/pytorch_Realtime_Multi-Person_Pose_Estimation.git my_pose_repo\n",
        "%cd my_pose_repo\n",
        "!pip install -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyBXfdPeqJRD",
        "outputId": "1f5b3ecd-091c-476f-8bee-6b8b409dfbe9"
      },
      "outputs": [],
      "source": [
        "# Create weight folder\n",
        "!mkdir -p model/weight\n",
        "\n",
        "# Download pretrained model\n",
        "!gdown https://drive.google.com/uc?id=1_g4z0c1m5y7TY9FXki8uTf3JEtkTSSPF \\\n",
        "       -O model/weight/pose_model.pth\n",
        "\n",
        "# Copy to repo root (expected by demo code)\n",
        "!cp model/weight/pose_model.pth pose_model.pth\n",
        "!ls -lh pose_model.pth\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYFMb6R_qMKq",
        "outputId": "e6aee6af-cc75-4c1a-eb0d-442754e54402"
      },
      "outputs": [],
      "source": [
        "%cd lib/pafprocess\n",
        "!sh make.sh\n",
        "%cd ../../\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uT3GZUK6qPrL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import subprocess\n",
        "from tqdm import tqdm\n",
        "\n",
        "PATCH_DIR = \"/content/patches\"\n",
        "OUTPUT_DIR = \"/content/patches_processed\"\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Load metadata\n",
        "with open(os.path.join(PATCH_DIR, \"metadata.json\"), \"r\") as f:\n",
        "    metadata = json.load(f)\n",
        "\n",
        "print(f\"Processing {metadata['num_patches']} patches...\")\n",
        "\n",
        "for patch in tqdm(metadata[\"patches\"]):\n",
        "    patch_name = patch[\"filename\"]\n",
        "    patch_path = os.path.join(PATCH_DIR, patch_name)\n",
        "    out_path = os.path.join(OUTPUT_DIR, patch_name)\n",
        "\n",
        "    # Replace demo input image with patch path\n",
        "    demo_file = \"demo/picture_demo.py\"\n",
        "\n",
        "    with open(demo_file, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    with open(demo_file, \"w\") as f:\n",
        "        for line in lines:\n",
        "            if \"test_image =\" in line:\n",
        "                f.write(f'test_image = \"{patch_path}\"\\n')\n",
        "            else:\n",
        "                f.write(line)\n",
        "\n",
        "    # Run inference\n",
        "    subprocess.run(\n",
        "        [\"python\", \"demo/picture_demo.py\"],\n",
        "        stdout=subprocess.DEVNULL,\n",
        "        stderr=subprocess.DEVNULL\n",
        "    )\n",
        "\n",
        "    # Save output as processed patch\n",
        "    if os.path.exists(\"result.png\"):\n",
        "        result = cv2.imread(\"result.png\")\n",
        "        cv2.imwrite(out_path, result)\n",
        "    else:\n",
        "        print(f\"⚠️ No result for {patch_name}, copying original\")\n",
        "        cv2.imwrite(out_path, cv2.imread(patch_path))\n",
        "\n",
        "# Copy metadata unchanged\n",
        "with open(os.path.join(OUTPUT_DIR, \"metadata.json\"), \"w\") as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "print(\"✅ Patch inference complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qAeV5cb-qSyH",
        "outputId": "bb43b0e0-26b8-49fc-c6e1-225322390de8"
      },
      "outputs": [],
      "source": [
        "# Assuming reconstruct_from_patches is already defined (from your code)\n",
        "\n",
        "reconstructed, coverage, meta = reconstruct_from_patches(\n",
        "    patch_dir=\"/content/patches_processed\",\n",
        "    show_result=True\n",
        ")\n",
        "\n",
        "cv2.imwrite(\"reconstructed_pose.png\", reconstructed)\n",
        "print(\"Saved reconstructed_pose.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 905
        },
        "id": "KNmkCGdWs6i9",
        "outputId": "78489cad-c32c-4574-9b6a-a9cb23255146"
      },
      "outputs": [],
      "source": [
        "# ===== CONFIG =====\n",
        "IMAGE_PATH = \"/content/img0.png\"   # <-- CHANGE THIS\n",
        "REPO_DIR = \"/content/my_pose_repo\"\n",
        "\n",
        "# ==================\n",
        "import os\n",
        "import cv2\n",
        "from IPython.display import Image, display\n",
        "import subprocess\n",
        "\n",
        "os.chdir(REPO_DIR)\n",
        "\n",
        "# --- Update demo script to point to image ---\n",
        "demo_file = \"demo/picture_demo.py\"\n",
        "\n",
        "with open(demo_file, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "with open(demo_file, \"w\") as f:\n",
        "    for line in lines:\n",
        "        if \"test_image =\" in line:\n",
        "            f.write(f'test_image = \"{IMAGE_PATH}\"\\n')\n",
        "        else:\n",
        "            f.write(line)\n",
        "\n",
        "# --- Run inference ---\n",
        "subprocess.run(\n",
        "    [\"python\", \"demo/picture_demo.py\"],\n",
        "    stdout=subprocess.DEVNULL,\n",
        "    stderr=subprocess.DEVNULL\n",
        ")\n",
        "\n",
        "# --- Display result ---\n",
        "if os.path.exists(\"result.png\"):\n",
        "    print(\"✅ Pose estimation result:\")\n",
        "    display(Image(\"result.png\"))\n",
        "else:\n",
        "    print(\"❌ result.png not found — inference failed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvQyBTS9LCzp"
      },
      "source": [
        "# Patching by Depth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UptvMyHoOgcI"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate pillow opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208,
          "referenced_widgets": [
            "5bf61ef428254f98bf8a02965a9c60e9",
            "855b12d8af4e48e6afa581f5adb0660d",
            "beb78ba1ca744249932be4b993cb7cbe",
            "049f1f8ba3e243fbb51cfb6f209254ef",
            "9a0913e6adbf4aa39dee028888cc3ed3",
            "78466f41fef34176abefc1353e98c06d",
            "1865c50c41774197932d9726dc3d6abe",
            "a0d15d80d3fb42e4ba800a20eaee398a",
            "cad15255044b4382b3dcdb721b8da121",
            "f288c5b44fdb4e78a267bcbdef7b5167",
            "06e47d9953474a8ca57d0977a3502634"
          ]
        },
        "id": "2OgAjXYxSCpe",
        "outputId": "6e94e8be-d8b3-4d66-d530-fa44274c705f"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load depth model once\n",
        "depth_pipe = pipeline(task=\"depth-estimation\", model=\"depth-anything/Depth-Anything-V2-Small-hf\")\n",
        "print(\"✅ Depth model loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKRuRVZzSLKH",
        "outputId": "31c3d3cf-0344-40e4-8bee-0f0374bf876a"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "from PIL import Image\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "#              DEPTH COMPUTATION\n",
        "# ============================================================\n",
        "\n",
        "def compute_depth_map(image_bgr, depth_pipe):\n",
        "    \"\"\"\n",
        "    Run depth estimation on a BGR image (OpenCV format).\n",
        "\n",
        "    Returns:\n",
        "      depth_norm: numpy array [0, 1], where 1 = farthest (deepest)\n",
        "      depth_raw:  numpy array, raw predicted depth values\n",
        "    \"\"\"\n",
        "    # Convert BGR (OpenCV) → RGB (PIL)\n",
        "    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
        "    pil_image = Image.fromarray(image_rgb)\n",
        "\n",
        "    # Run inference\n",
        "    result = depth_pipe(pil_image)\n",
        "    depth_tensor = result[\"predicted_depth\"]  # torch tensor\n",
        "\n",
        "    # Convert to numpy\n",
        "    depth_raw = depth_tensor.squeeze().cpu().numpy()  # HxW\n",
        "\n",
        "    # Resize to match original image dimensions (model may output different size)\n",
        "    h, w = image_bgr.shape[:2]\n",
        "    if depth_raw.shape[0] != h or depth_raw.shape[1] != w:\n",
        "        depth_raw = cv2.resize(depth_raw, (w, h), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "    # Normalize to [0, 1]\n",
        "    d_min, d_max = depth_raw.min(), depth_raw.max()\n",
        "    if d_max - d_min > 1e-8:\n",
        "        depth_norm = 1.0 - (depth_raw - d_min) / (d_max - d_min)\n",
        "    else:\n",
        "        depth_norm = np.zeros_like(depth_raw)\n",
        "\n",
        "    return depth_norm, depth_raw\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "#         DEPTH-BASED ADAPTIVE SUBDIVISION\n",
        "# ============================================================\n",
        "\n",
        "def compute_mean_depth(depth_map, x, y, w, h):\n",
        "    region = depth_map[y:y+h, x:x+w]\n",
        "    return np.mean(region)\n",
        "\n",
        "\n",
        "def compute_depth_variance(depth_map, x, y, w, h):\n",
        "    region = depth_map[y:y+h, x:x+w]\n",
        "    return np.var(region)\n",
        "\n",
        "\n",
        "def adaptive_subdivide_depth(depth_map, x, y, w, h,\n",
        "                              depth_threshold=0.4,\n",
        "                              min_patch_width=32,\n",
        "                              max_patch_width=256,\n",
        "                              aspect_ratio=2.0):\n",
        "    \"\"\"\n",
        "    Recursively subdivide based on depth.\n",
        "      - Deep regions (far away, value closer to 1) → smaller patches\n",
        "      - Shallow regions (close, value closer to 0) → keep large\n",
        "      - High depth variance → subdivide (mixed near/far)\n",
        "    \"\"\"\n",
        "    min_patch_height = int(min_patch_width * aspect_ratio)\n",
        "\n",
        "    if w <= min_patch_width or h <= min_patch_height:\n",
        "        return [(x, y, w, h)]\n",
        "\n",
        "    mean_depth = compute_mean_depth(depth_map, x, y, w, h)\n",
        "    depth_var = compute_depth_variance(depth_map, x, y, w, h)\n",
        "\n",
        "    mid_w = max_patch_width // 2\n",
        "    mid_h = int(mid_w * aspect_ratio)\n",
        "\n",
        "    # Don't subdivide if shallow and uniform\n",
        "    if mean_depth <= depth_threshold and depth_var < 0.02:\n",
        "        return [(x, y, w, h)]\n",
        "\n",
        "    if (w <= mid_w and h <= mid_h and\n",
        "        mean_depth <= depth_threshold * 1.3 and depth_var < 0.03):\n",
        "        return [(x, y, w, h)]\n",
        "\n",
        "    # Subdivide into 4 quadrants\n",
        "    half_w = w // 2\n",
        "    half_h = h // 2\n",
        "    right_w = w - half_w\n",
        "    bottom_h = h - half_h\n",
        "\n",
        "    patches = []\n",
        "    patches += adaptive_subdivide_depth(depth_map, x, y,\n",
        "                                         half_w, half_h,\n",
        "                                         depth_threshold, min_patch_width,\n",
        "                                         max_patch_width, aspect_ratio)\n",
        "    patches += adaptive_subdivide_depth(depth_map, x + half_w, y,\n",
        "                                         right_w, half_h,\n",
        "                                         depth_threshold, min_patch_width,\n",
        "                                         max_patch_width, aspect_ratio)\n",
        "    patches += adaptive_subdivide_depth(depth_map, x, y + half_h,\n",
        "                                         half_w, bottom_h,\n",
        "                                         depth_threshold, min_patch_width,\n",
        "                                         max_patch_width, aspect_ratio)\n",
        "    patches += adaptive_subdivide_depth(depth_map, x + half_w, y + half_h,\n",
        "                                         right_w, bottom_h,\n",
        "                                         depth_threshold, min_patch_width,\n",
        "                                         max_patch_width, aspect_ratio)\n",
        "    return patches\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "#              EDGE-AWARE + DEPTH-AWARE MERGING\n",
        "# ============================================================\n",
        "\n",
        "def compute_edge_map(gray):\n",
        "    blurred = cv2.GaussianBlur(gray, (3, 3), 0)\n",
        "    edges = cv2.Canny(blurred, 50, 150)\n",
        "    return edges\n",
        "\n",
        "\n",
        "def get_shared_boundary(patch_a, patch_b):\n",
        "    ax, ay, aw, ah = patch_a\n",
        "    bx, by, bw, bh = patch_b\n",
        "    tolerance = 2\n",
        "\n",
        "    if abs((ay + ah) - by) <= tolerance:\n",
        "        ol = max(ax, bx)\n",
        "        or_ = min(ax + aw, bx + bw)\n",
        "        if or_ - ol > tolerance:\n",
        "            return ('horizontal', ol, or_, ay + ah)\n",
        "\n",
        "    if abs((by + bh) - ay) <= tolerance:\n",
        "        ol = max(ax, bx)\n",
        "        or_ = min(ax + aw, bx + bw)\n",
        "        if or_ - ol > tolerance:\n",
        "            return ('horizontal', ol, or_, by + bh)\n",
        "\n",
        "    if abs((ax + aw) - bx) <= tolerance:\n",
        "        ot = max(ay, by)\n",
        "        ob = min(ay + ah, by + bh)\n",
        "        if ob - ot > tolerance:\n",
        "            return ('vertical', ot, ob, ax + aw)\n",
        "\n",
        "    if abs((bx + bw) - ax) <= tolerance:\n",
        "        ot = max(ay, by)\n",
        "        ob = min(ay + ah, by + bh)\n",
        "        if ob - ot > tolerance:\n",
        "            return ('vertical', ot, ob, bx + bw)\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "def compute_boundary_edge_density(edges, boundary_info, band_width=5):\n",
        "    orientation, start, end, line_pos = boundary_info\n",
        "    h, w = edges.shape\n",
        "\n",
        "    if orientation == 'horizontal':\n",
        "        rs = max(0, line_pos - band_width)\n",
        "        re = min(h, line_pos + band_width)\n",
        "        band = edges[rs:re, start:end]\n",
        "    else:\n",
        "        cs = max(0, line_pos - band_width)\n",
        "        ce = min(w, line_pos + band_width)\n",
        "        band = edges[start:end, cs:ce]\n",
        "\n",
        "    if band.size == 0:\n",
        "        return 0.0\n",
        "    return np.sum(band > 0) / band.size\n",
        "\n",
        "\n",
        "def merged_patch_bbox(patch_a, patch_b):\n",
        "    ax, ay, aw, ah = patch_a\n",
        "    bx, by, bw, bh = patch_b\n",
        "    new_x = min(ax, bx)\n",
        "    new_y = min(ay, by)\n",
        "    new_x2 = max(ax + aw, bx + bw)\n",
        "    new_y2 = max(ay + ah, by + bh)\n",
        "    return (new_x, new_y, new_x2 - new_x, new_y2 - new_y)\n",
        "\n",
        "\n",
        "def should_merge_depth(patch_a, patch_b, depth_map, edges,\n",
        "                        max_patch_width, max_patch_height,\n",
        "                        depth_similarity_threshold=0.1,\n",
        "                        edge_density_threshold=0.15,\n",
        "                        band_width=5):\n",
        "    boundary = get_shared_boundary(patch_a, patch_b)\n",
        "    if boundary is None:\n",
        "        return False, None, 0.0\n",
        "\n",
        "    merged = merged_patch_bbox(patch_a, patch_b)\n",
        "    _, _, mw, mh = merged\n",
        "    if mw > max_patch_width or mh > max_patch_height:\n",
        "        return False, boundary, 0.0\n",
        "\n",
        "    depth_a = compute_mean_depth(depth_map, *patch_a)\n",
        "    depth_b = compute_mean_depth(depth_map, *patch_b)\n",
        "    depth_diff = abs(depth_a - depth_b)\n",
        "    avg_depth = (depth_a + depth_b) / 2.0\n",
        "\n",
        "    # Merge if both shallow and similar depth\n",
        "    if depth_diff < depth_similarity_threshold and avg_depth < 0.5:\n",
        "        return True, boundary, 1.0 - depth_diff\n",
        "\n",
        "    # Also allow merge if no strong edge and depth is close enough\n",
        "    edge_density = compute_boundary_edge_density(edges, boundary, band_width)\n",
        "    if edge_density < edge_density_threshold and depth_diff < depth_similarity_threshold * 2:\n",
        "        return True, boundary, 1.0 - depth_diff\n",
        "\n",
        "    return False, boundary, 0.0\n",
        "\n",
        "\n",
        "def depth_aware_merge(patches, depth_map, edges,\n",
        "                       max_patch_width, max_patch_height,\n",
        "                       depth_similarity_threshold=0.1,\n",
        "                       edge_density_threshold=0.15,\n",
        "                       band_width=5,\n",
        "                       max_iterations=50):\n",
        "    merged_patches = list(patches)\n",
        "    total_merges = 0\n",
        "\n",
        "    for iteration in range(max_iterations):\n",
        "        best_merge = None\n",
        "        best_score = 0.0\n",
        "\n",
        "        for i in range(len(merged_patches)):\n",
        "            for j in range(i + 1, len(merged_patches)):\n",
        "                do_merge, boundary, score = should_merge_depth(\n",
        "                    merged_patches[i], merged_patches[j],\n",
        "                    depth_map, edges,\n",
        "                    max_patch_width, max_patch_height,\n",
        "                    depth_similarity_threshold,\n",
        "                    edge_density_threshold, band_width\n",
        "                )\n",
        "                if do_merge and score > best_score:\n",
        "                    best_merge = (i, j)\n",
        "                    best_score = score\n",
        "\n",
        "        if best_merge is None:\n",
        "            break\n",
        "\n",
        "        i, j = best_merge\n",
        "        new_patch = merged_patch_bbox(merged_patches[i], merged_patches[j])\n",
        "        merged_patches.pop(j)\n",
        "        merged_patches.pop(i)\n",
        "        merged_patches.append(new_patch)\n",
        "        total_merges += 1\n",
        "\n",
        "    print(f\"  Depth-aware merging: {total_merges} merges in {iteration+1} iterations\")\n",
        "    print(f\"  Patches: {len(patches)} → {len(merged_patches)}\")\n",
        "    return merged_patches\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "#         EXPORT & RECONSTRUCTION\n",
        "# ============================================================\n",
        "\n",
        "def export_patches(image, patches, patch_info, output_dir='patches',\n",
        "                   image_path=None):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    height, width = image.shape[:2]\n",
        "    channels = image.shape[2] if len(image.shape) == 3 else 1\n",
        "\n",
        "    metadata = {\n",
        "        'original_image': image_path if image_path else 'unknown',\n",
        "        'original_width': width,\n",
        "        'original_height': height,\n",
        "        'original_channels': channels,\n",
        "        'num_patches': len(patches),\n",
        "        'patches': []\n",
        "    }\n",
        "\n",
        "    for i, (x, y, w, h) in enumerate(patches):\n",
        "        patch_img = image[y:y+h, x:x+w].copy()\n",
        "        filename = f\"patch_{i:03d}.png\"\n",
        "        filepath = os.path.join(output_dir, filename)\n",
        "        cv2.imwrite(filepath, patch_img)\n",
        "\n",
        "        info = None\n",
        "        for p in patch_info:\n",
        "            if p['x'] == x and p['y'] == y and p['w'] == w and p['h'] == h:\n",
        "                info = p\n",
        "                break\n",
        "\n",
        "        patch_meta = {\n",
        "            'index': i,\n",
        "            'filename': filename,\n",
        "            'x': int(x),\n",
        "            'y': int(y),\n",
        "            'w': int(w),\n",
        "            'h': int(h),\n",
        "            'mean_depth': float(info['mean_depth']) if info else 0.0,\n",
        "            'depth_variance': float(info['depth_variance']) if info else 0.0\n",
        "        }\n",
        "        metadata['patches'].append(patch_meta)\n",
        "\n",
        "    meta_path = os.path.join(output_dir, 'metadata.json')\n",
        "    with open(meta_path, 'w') as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "\n",
        "    print(f\"\\n  Exported {len(patches)} patches to '{output_dir}/'\")\n",
        "\n",
        "    total_size = sum(\n",
        "        os.path.getsize(os.path.join(output_dir, f\"patch_{i:03d}.png\"))\n",
        "        for i in range(len(patches))\n",
        "    )\n",
        "    meta_size = os.path.getsize(meta_path)\n",
        "    total_size += meta_size\n",
        "    print(f\"  Total size: {total_size / 1024:.1f} KB\")\n",
        "\n",
        "    return metadata\n",
        "\n",
        "\n",
        "def reconstruct_from_patches(patch_dir='patches', show_result=True):\n",
        "    meta_path = os.path.join(patch_dir, 'metadata.json')\n",
        "    with open(meta_path, 'r') as f:\n",
        "        metadata = json.load(f)\n",
        "\n",
        "    orig_w = metadata['original_width']\n",
        "    orig_h = metadata['original_height']\n",
        "    orig_c = metadata['original_channels']\n",
        "\n",
        "    print(f\"Reconstructing: {orig_w}x{orig_h}x{orig_c} from {metadata['num_patches']} patches\")\n",
        "\n",
        "    if orig_c == 1:\n",
        "        canvas = np.zeros((orig_h, orig_w), dtype=np.uint8)\n",
        "    else:\n",
        "        canvas = np.zeros((orig_h, orig_w, orig_c), dtype=np.uint8)\n",
        "\n",
        "    coverage = np.zeros((orig_h, orig_w), dtype=np.int32)\n",
        "\n",
        "    for patch_meta in metadata['patches']:\n",
        "        x, y, w, h = patch_meta['x'], patch_meta['y'], patch_meta['w'], patch_meta['h']\n",
        "        patch_path = os.path.join(patch_dir, patch_meta['filename'])\n",
        "\n",
        "        patch_img = cv2.imread(patch_path, cv2.IMREAD_UNCHANGED)\n",
        "        if patch_img is None:\n",
        "            print(f\"  WARNING: Could not read {patch_path}\")\n",
        "            continue\n",
        "\n",
        "        if orig_c == 1 and len(patch_img.shape) == 3:\n",
        "            patch_img = cv2.cvtColor(patch_img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        patch_img = patch_img[:h, :w]\n",
        "        canvas[y:y+h, x:x+w] = patch_img\n",
        "        coverage[y:y+h, x:x+w] += 1\n",
        "\n",
        "    uncovered = np.sum(coverage == 0)\n",
        "    total_pixels = orig_w * orig_h\n",
        "    print(f\"  Coverage: {np.sum(coverage == 1)}/{total_pixels} \"\n",
        "          f\"({100 * np.sum(coverage == 1) / total_pixels:.2f}%)\")\n",
        "    if uncovered > 0:\n",
        "        print(f\"  ⚠️ Uncovered: {uncovered} pixels\")\n",
        "\n",
        "    if show_result:\n",
        "        cv2_imshow(canvas)\n",
        "\n",
        "    return canvas, coverage, metadata\n",
        "\n",
        "\n",
        "def verify_reconstruction(original_image_path, patch_dir='patches'):\n",
        "    original = cv2.imread(original_image_path)\n",
        "    reconstructed, _, _ = reconstruct_from_patches(patch_dir, show_result=False)\n",
        "\n",
        "    if original.shape != reconstructed.shape:\n",
        "        print(f\"  SHAPE MISMATCH: {original.shape} vs {reconstructed.shape}\")\n",
        "        return False\n",
        "\n",
        "    diff = cv2.absdiff(original, reconstructed)\n",
        "    max_diff = np.max(diff)\n",
        "\n",
        "    if max_diff == 0:\n",
        "        print(\"  ✅ PERFECT reconstruction!\")\n",
        "    else:\n",
        "        print(f\"  ⚠️ Max diff: {max_diff}, Mean: {np.mean(diff):.6f}\")\n",
        "\n",
        "    h, w = original.shape[:2]\n",
        "    scale = min(1.0, 800 / w)\n",
        "    orig_s = cv2.resize(original, None, fx=scale, fy=scale) if scale < 1 else original\n",
        "    recon_s = cv2.resize(reconstructed, None, fx=scale, fy=scale) if scale < 1 else reconstructed\n",
        "    comparison = np.hstack([orig_s, recon_s])\n",
        "    cv2.putText(comparison, \"Original\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "    cv2.putText(comparison, \"Reconstructed\", (orig_s.shape[1] + 10, 30),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
        "    cv2_imshow(comparison)\n",
        "    return max_diff == 0\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "#         VISUALIZATION\n",
        "# ============================================================\n",
        "\n",
        "def visualize_depth_patches(image, depth_norm, patches, patch_info, patches_before):\n",
        "    def depth_color(ratio):\n",
        "        b = int(255 * (1 - ratio))\n",
        "        r = int(255 * ratio)\n",
        "        return (b, 0, r)\n",
        "\n",
        "    # Depth map\n",
        "    depth_vis = (depth_norm * 255).astype(np.uint8)\n",
        "    depth_colored = cv2.applyColorMap(depth_vis, cv2.COLORMAP_INFERNO)\n",
        "    print(\"\\n--- Depth Map ---\")\n",
        "    cv2_imshow(depth_colored)\n",
        "\n",
        "    # Patches colored by depth\n",
        "    patches_vis = image.copy()\n",
        "    for p in patch_info:\n",
        "        x, y, w, h = p['x'], p['y'], p['w'], p['h']\n",
        "        cv2.rectangle(patches_vis, (x, y), (x+w, y+h),\n",
        "                      depth_color(min(p['mean_depth'], 1.0)), 2)\n",
        "    print(f\"\\n--- Patches ({patches_before}→{len(patches)}) by depth ---\")\n",
        "    cv2_imshow(patches_vis)\n",
        "\n",
        "    # Overlay\n",
        "    alpha_layer = image.copy()\n",
        "    for p in patch_info:\n",
        "        x, y, w, h = p['x'], p['y'], p['w'], p['h']\n",
        "        cv2.rectangle(alpha_layer, (x, y), (x+w, y+h),\n",
        "                      depth_color(min(p['mean_depth'], 1.0)), -1)\n",
        "    result = cv2.addWeighted(image, 0.6, alpha_layer, 0.4, 0)\n",
        "    for p in patch_info:\n",
        "        cv2.rectangle(result, (p['x'], p['y']),\n",
        "                      (p['x']+p['w'], p['y']+p['h']), (255, 255, 255), 1)\n",
        "    print(\"\\n--- Depth Overlay ---\")\n",
        "    cv2_imshow(result)\n",
        "\n",
        "    # Sizes\n",
        "    size_vis = image.copy()\n",
        "    areas = [p['area'] for p in patch_info]\n",
        "    min_a, max_a = min(areas), max(areas)\n",
        "    ar = max(max_a - min_a, 1)\n",
        "    for p in patch_info:\n",
        "        x, y, w, h = p['x'], p['y'], p['w'], p['h']\n",
        "        ratio = (p['area'] - min_a) / ar\n",
        "        cv2.rectangle(size_vis, (x, y), (x+w, y+h),\n",
        "                      (int(255*ratio), 50, int(255*(1-ratio))), 2)\n",
        "        if w > 40 and h > 30:\n",
        "            cv2.putText(size_vis, f\"{w}x{h}\", (x+3, y+15),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1)\n",
        "    print(\"\\n--- Patch Sizes ---\")\n",
        "    cv2_imshow(size_vis)\n",
        "\n",
        "    depths = [p['mean_depth'] for p in patch_info]\n",
        "    print(f\"\\n--- Stats ---\")\n",
        "    print(f\"Patches: {len(patches)} | Merged: {patches_before - len(patches)}\")\n",
        "    print(f\"Depth range: [{min(depths):.3f}, {max(depths):.3f}]\")\n",
        "    print(f\"Avg aspect: {np.mean([p['aspect'] for p in patch_info]):.2f}\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "#                    MAIN PIPELINE\n",
        "# ============================================================\n",
        "\n",
        "def generate_depth_adaptive_patches(image_path,\n",
        "                                     depth_pipe,\n",
        "                                     depth_threshold=0.4,\n",
        "                                     min_patch_width=32,\n",
        "                                     max_patch_width=256,\n",
        "                                     aspect_ratio=2.0,\n",
        "                                     depth_similarity_threshold=0.1,\n",
        "                                     edge_density_threshold=0.15,\n",
        "                                     edge_band_width=5,\n",
        "                                     output_dir='patches_depth'):\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        raise ValueError(f\"Image not found: {image_path}\")\n",
        "\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    height, width = gray.shape\n",
        "    max_patch_height = int(max_patch_width * aspect_ratio)\n",
        "\n",
        "    print(f\"Image: {width}x{height}\")\n",
        "    print(f\"Patch range: {min_patch_width}x{int(min_patch_width*aspect_ratio)} \"\n",
        "          f\"→ {max_patch_width}x{max_patch_height}\")\n",
        "    print(f\"Depth threshold: {depth_threshold}\")\n",
        "\n",
        "    # Step 1: Depth map\n",
        "    print(\"\\n[Step 1] Computing depth...\")\n",
        "    depth_norm, depth_raw = compute_depth_map(image, depth_pipe)\n",
        "    print(f\"  Raw range: [{depth_raw.min():.2f}, {depth_raw.max():.2f}]\")\n",
        "    print(f\"  Mean (norm): {depth_norm.mean():.3f}\")\n",
        "\n",
        "    # Step 2: Subdivide\n",
        "    print(\"\\n[Step 2] Depth-adaptive subdivision...\")\n",
        "    patches = adaptive_subdivide_depth(\n",
        "        depth_norm, 0, 0, width, height,\n",
        "        depth_threshold, min_patch_width, max_patch_width, aspect_ratio\n",
        "    )\n",
        "    print(f\"  Patches: {len(patches)}\")\n",
        "\n",
        "    # Step 3: Merge\n",
        "    print(\"\\n[Step 3] Depth-aware merging...\")\n",
        "    edges = compute_edge_map(gray)\n",
        "    patches_before = len(patches)\n",
        "    patches = depth_aware_merge(\n",
        "        patches, depth_norm, edges,\n",
        "        max_patch_width, max_patch_height,\n",
        "        depth_similarity_threshold, edge_density_threshold, edge_band_width\n",
        "    )\n",
        "\n",
        "    # Step 4: Patch info\n",
        "    patch_info = []\n",
        "    for (x, y, w, h) in patches:\n",
        "        patch_info.append({\n",
        "            'x': x, 'y': y, 'w': w, 'h': h,\n",
        "            'mean_depth': compute_mean_depth(depth_norm, x, y, w, h),\n",
        "            'depth_variance': compute_depth_variance(depth_norm, x, y, w, h),\n",
        "            'area': w * h,\n",
        "            'aspect': h / max(w, 1)\n",
        "        })\n",
        "    patch_info.sort(key=lambda p: p['mean_depth'], reverse=True)\n",
        "\n",
        "    # Step 5: Visualize\n",
        "    visualize_depth_patches(image, depth_norm, patches, patch_info, patches_before)\n",
        "\n",
        "    # Step 6: Export\n",
        "    print(\"\\n[Step 6] Exporting...\")\n",
        "    metadata = export_patches(image, patches, patch_info,\n",
        "                              output_dir=output_dir, image_path=image_path)\n",
        "\n",
        "    # Step 7: Verify\n",
        "    print(\"\\n[Step 7] Verifying...\")\n",
        "    verify_reconstruction(image_path, patch_dir=output_dir)\n",
        "\n",
        "    return patches, patch_info, depth_norm, metadata\n",
        "\n",
        "\n",
        "print(\"✅ Depth-based patching pipeline ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_pvk9Bq6SODl",
        "outputId": "7d05be7a-4556-4144-8216-3bdc9fafe777"
      },
      "outputs": [],
      "source": [
        "IMAGE_PATH = '/content/img0.png'  # ← change this\n",
        "\n",
        "patches, patch_info, depth_map, metadata = generate_depth_adaptive_patches(\n",
        "    image_path=IMAGE_PATH,\n",
        "    depth_pipe=depth_pipe,           # from Cell 1\n",
        "    depth_threshold=0.7,             # lower = more subdivision\n",
        "    min_patch_width=32,\n",
        "    max_patch_width=256,\n",
        "    aspect_ratio=2.0,\n",
        "    depth_similarity_threshold=0.1,\n",
        "    edge_density_threshold=0.15,\n",
        "    output_dir='/content/patches_depth'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cf5jCoF3aXfr",
        "outputId": "f0fd54bc-3291-4f4e-cafb-3972fbfef82a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import subprocess\n",
        "from tqdm import tqdm\n",
        "\n",
        "PATCH_DIR = \"/content/patches_depth\"\n",
        "OUTPUT_DIR = \"/content/patches_depth_processed\"\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Load metadata\n",
        "with open(os.path.join(PATCH_DIR, \"metadata.json\"), \"r\") as f:\n",
        "    metadata = json.load(f)\n",
        "\n",
        "print(f\"Processing {metadata['num_patches']} patches...\")\n",
        "\n",
        "for patch in tqdm(metadata[\"patches\"]):\n",
        "    patch_name = patch[\"filename\"]\n",
        "    patch_path = os.path.join(PATCH_DIR, patch_name)\n",
        "    out_path = os.path.join(OUTPUT_DIR, patch_name)\n",
        "\n",
        "    # Replace demo input image with patch path\n",
        "    demo_file = \"demo/picture_demo.py\"\n",
        "\n",
        "    with open(demo_file, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    with open(demo_file, \"w\") as f:\n",
        "        for line in lines:\n",
        "            if \"test_image =\" in line:\n",
        "                f.write(f'test_image = \"{patch_path}\"\\n')\n",
        "            else:\n",
        "                f.write(line)\n",
        "\n",
        "    # Run inference\n",
        "    subprocess.run(\n",
        "        [\"python\", \"demo/picture_demo.py\"],\n",
        "        stdout=subprocess.DEVNULL,\n",
        "        stderr=subprocess.DEVNULL\n",
        "    )\n",
        "\n",
        "    # Save output as processed patch\n",
        "    if os.path.exists(\"result.png\"):\n",
        "        result = cv2.imread(\"result.png\")\n",
        "        cv2.imwrite(out_path, result)\n",
        "    else:\n",
        "        print(f\"⚠️ No result for {patch_name}, copying original\")\n",
        "        cv2.imwrite(out_path, cv2.imread(patch_path))\n",
        "\n",
        "# Copy metadata unchanged\n",
        "with open(os.path.join(OUTPUT_DIR, \"metadata.json\"), \"w\") as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "print(\"✅ Patch inference complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "h354yQWSaXfs",
        "outputId": "d7db8f09-626a-4083-b22d-4c905c51bf9d"
      },
      "outputs": [],
      "source": [
        "# Assuming reconstruct_from_patches is already defined (from your code)\n",
        "\n",
        "reconstructed, coverage, meta = reconstruct_from_patches(\n",
        "    patch_dir=\"/content/patches_depth_processed\",\n",
        "    show_result=True\n",
        ")\n",
        "\n",
        "cv2.imwrite(\"reconstructed_pose.png\", reconstructed)\n",
        "print(\"Saved reconstructed_pose.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHST2acwd6ow"
      },
      "source": [
        "# Hijab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "o3CufIZUd8MP",
        "outputId": "03730cd4-0745-4a52-fd7e-ef0e9aa37605"
      },
      "outputs": [],
      "source": [
        "# ===== CONFIG =====\n",
        "IMAGE_PATH = \"/content/img3.jpg\"   # <-- CHANGE THIS\n",
        "REPO_DIR = \"/content/my_pose_repo\"\n",
        "\n",
        "# ==================\n",
        "import os\n",
        "import cv2\n",
        "from IPython.display import Image, display\n",
        "import subprocess\n",
        "\n",
        "os.chdir(REPO_DIR)\n",
        "\n",
        "# --- Update demo script to point to image ---\n",
        "demo_file = \"demo/picture_demo.py\"\n",
        "\n",
        "with open(demo_file, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "with open(demo_file, \"w\") as f:\n",
        "    for line in lines:\n",
        "        if \"test_image =\" in line:\n",
        "            f.write(f'test_image = \"{IMAGE_PATH}\"\\n')\n",
        "        else:\n",
        "            f.write(line)\n",
        "\n",
        "# --- Run inference ---\n",
        "subprocess.run(\n",
        "    [\"python\", \"demo/picture_demo.py\"],\n",
        "    stdout=subprocess.DEVNULL,\n",
        "    stderr=subprocess.DEVNULL\n",
        ")\n",
        "\n",
        "# --- Display result ---\n",
        "if os.path.exists(\"result.png\"):\n",
        "    print(\"✅ Pose estimation result:\")\n",
        "    display(Image(\"result.png\"))\n",
        "else:\n",
        "    print(\"❌ result.png not found — inference failed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "joZIUQLKfZOJ",
        "outputId": "c2cf0d9a-8f1f-4c5d-b28a-61b8dca3ba35"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "# input and output paths\n",
        "input_path = \"/content/img3.jpg\"\n",
        "output_path = \"output_negative.jpg\"\n",
        "\n",
        "# read image\n",
        "image = cv2.imread(input_path)\n",
        "\n",
        "# check if image loaded\n",
        "if image is None:\n",
        "    raise ValueError(\"Could not load image. Check the input path.\")\n",
        "\n",
        "# create negative (invert colors)\n",
        "negative = 255 - image\n",
        "\n",
        "# save result\n",
        "cv2.imwrite(output_path, negative)\n",
        "\n",
        "print(\"Negative image saved to:\", output_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "z0jslc3qfnh3",
        "outputId": "6e066b6b-8850-468b-8bb9-a83ebec364b0"
      },
      "outputs": [],
      "source": [
        "# ===== CONFIG =====\n",
        "IMAGE_PATH = \"output_negative.jpg\"   # <-- CHANGE THIS\n",
        "REPO_DIR = \"/content/my_pose_repo\"\n",
        "\n",
        "# ==================\n",
        "import os\n",
        "import cv2\n",
        "from IPython.display import Image, display\n",
        "import subprocess\n",
        "\n",
        "os.chdir(REPO_DIR)\n",
        "\n",
        "# --- Update demo script to point to image ---\n",
        "demo_file = \"demo/picture_demo.py\"\n",
        "\n",
        "with open(demo_file, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "with open(demo_file, \"w\") as f:\n",
        "    for line in lines:\n",
        "        if \"test_image =\" in line:\n",
        "            f.write(f'test_image = \"{IMAGE_PATH}\"\\n')\n",
        "        else:\n",
        "            f.write(line)\n",
        "\n",
        "# --- Run inference ---\n",
        "subprocess.run(\n",
        "    [\"python\", \"demo/picture_demo.py\"],\n",
        "    stdout=subprocess.DEVNULL,\n",
        "    stderr=subprocess.DEVNULL\n",
        ")\n",
        "\n",
        "# --- Display result ---\n",
        "if os.path.exists(\"result.png\"):\n",
        "    print(\"✅ Pose estimation result:\")\n",
        "    display(Image(\"result.png\"))\n",
        "else:\n",
        "    print(\"❌ result.png not found — inference failed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4YX8L-K4fzQ8",
        "outputId": "1a353007-bc3a-4782-c821-3534f17164ab"
      },
      "outputs": [],
      "source": [
        "# ===== CONFIG =====\n",
        "IMAGE_PATH = \"/content/img4.jpg\"   # <-- CHANGE THIS\n",
        "REPO_DIR = \"/content/my_pose_repo\"\n",
        "\n",
        "# ==================\n",
        "import os\n",
        "import cv2\n",
        "from IPython.display import Image, display\n",
        "import subprocess\n",
        "\n",
        "os.chdir(REPO_DIR)\n",
        "\n",
        "# --- Update demo script to point to image ---\n",
        "demo_file = \"demo/picture_demo.py\"\n",
        "\n",
        "with open(demo_file, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "with open(demo_file, \"w\") as f:\n",
        "    for line in lines:\n",
        "        if \"test_image =\" in line:\n",
        "            f.write(f'test_image = \"{IMAGE_PATH}\"\\n')\n",
        "        else:\n",
        "            f.write(line)\n",
        "\n",
        "# --- Run inference ---\n",
        "subprocess.run(\n",
        "    [\"python\", \"demo/picture_demo.py\"],\n",
        "    stdout=subprocess.DEVNULL,\n",
        "    stderr=subprocess.DEVNULL\n",
        ")\n",
        "\n",
        "# --- Display result ---\n",
        "if os.path.exists(\"result.png\"):\n",
        "    print(\"✅ Pose estimation result:\")\n",
        "    display(Image(\"result.png\"))\n",
        "else:\n",
        "    print(\"❌ result.png not found — inference failed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 927
        },
        "id": "5KB3QcAxgkkd",
        "outputId": "c3ef5268-0bc7-487d-cc58-9fcca80dc324"
      },
      "outputs": [],
      "source": [
        "# ===== CONFIG =====\n",
        "IMAGE_PATH = \"/content/img5.jpg\"   # <-- CHANGE THIS\n",
        "REPO_DIR = \"/content/my_pose_repo\"\n",
        "\n",
        "# ==================\n",
        "import os\n",
        "import cv2\n",
        "from IPython.display import Image, display\n",
        "import subprocess\n",
        "\n",
        "os.chdir(REPO_DIR)\n",
        "\n",
        "# --- Update demo script to point to image ---\n",
        "demo_file = \"demo/picture_demo.py\"\n",
        "\n",
        "with open(demo_file, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "with open(demo_file, \"w\") as f:\n",
        "    for line in lines:\n",
        "        if \"test_image =\" in line:\n",
        "            f.write(f'test_image = \"{IMAGE_PATH}\"\\n')\n",
        "        else:\n",
        "            f.write(line)\n",
        "\n",
        "# --- Run inference ---\n",
        "subprocess.run(\n",
        "    [\"python\", \"demo/picture_demo.py\"],\n",
        "    stdout=subprocess.DEVNULL,\n",
        "    stderr=subprocess.DEVNULL\n",
        ")\n",
        "\n",
        "# --- Display result ---\n",
        "if os.path.exists(\"result.png\"):\n",
        "    print(\"✅ Pose estimation result:\")\n",
        "    display(Image(\"result.png\"))\n",
        "else:\n",
        "    print(\"❌ result.png not found — inference failed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "TymqRd2WnGFI",
        "outputId": "28dab283-1a7d-48e7-b1da-e3c5f5bf3e0d"
      },
      "outputs": [],
      "source": [
        "# ===== CONFIG =====\n",
        "IMAGE_PATH = \"/content/img6.jpg\"   # <-- CHANGE THIS\n",
        "REPO_DIR = \"/content/my_pose_repo\"\n",
        "\n",
        "# ==================\n",
        "import os\n",
        "import cv2\n",
        "from IPython.display import Image, display\n",
        "import subprocess\n",
        "\n",
        "os.chdir(REPO_DIR)\n",
        "\n",
        "# --- Update demo script to point to image ---\n",
        "demo_file = \"demo/picture_demo.py\"\n",
        "\n",
        "with open(demo_file, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "with open(demo_file, \"w\") as f:\n",
        "    for line in lines:\n",
        "        if \"test_image =\" in line:\n",
        "            f.write(f'test_image = \"{IMAGE_PATH}\"\\n')\n",
        "        else:\n",
        "            f.write(line)\n",
        "\n",
        "# --- Run inference ---\n",
        "subprocess.run(\n",
        "    [\"python\", \"demo/picture_demo.py\"],\n",
        "    stdout=subprocess.DEVNULL,\n",
        "    stderr=subprocess.DEVNULL\n",
        ")\n",
        "\n",
        "# --- Display result ---\n",
        "if os.path.exists(\"result.png\"):\n",
        "    print(\"✅ Pose estimation result:\")\n",
        "    display(Image(\"result.png\"))\n",
        "else:\n",
        "    print(\"❌ result.png not found — inference failed\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "dTP4-WUczcfS"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "049f1f8ba3e243fbb51cfb6f209254ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f288c5b44fdb4e78a267bcbdef7b5167",
            "placeholder": "​",
            "style": "IPY_MODEL_06e47d9953474a8ca57d0977a3502634",
            "value": " 287/287 [00:00&lt;00:00, 762.78it/s, Materializing param=neck.reassemble_stage.layers.3.resize.weight]"
          }
        },
        "06e47d9953474a8ca57d0977a3502634": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1865c50c41774197932d9726dc3d6abe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5bf61ef428254f98bf8a02965a9c60e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_855b12d8af4e48e6afa581f5adb0660d",
              "IPY_MODEL_beb78ba1ca744249932be4b993cb7cbe",
              "IPY_MODEL_049f1f8ba3e243fbb51cfb6f209254ef"
            ],
            "layout": "IPY_MODEL_9a0913e6adbf4aa39dee028888cc3ed3"
          }
        },
        "78466f41fef34176abefc1353e98c06d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "855b12d8af4e48e6afa581f5adb0660d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78466f41fef34176abefc1353e98c06d",
            "placeholder": "​",
            "style": "IPY_MODEL_1865c50c41774197932d9726dc3d6abe",
            "value": "Loading weights: 100%"
          }
        },
        "9a0913e6adbf4aa39dee028888cc3ed3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0d15d80d3fb42e4ba800a20eaee398a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "beb78ba1ca744249932be4b993cb7cbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0d15d80d3fb42e4ba800a20eaee398a",
            "max": 287,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cad15255044b4382b3dcdb721b8da121",
            "value": 287
          }
        },
        "cad15255044b4382b3dcdb721b8da121": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f288c5b44fdb4e78a267bcbdef7b5167": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
